---
title: "Estimation of Contract Schedules using Constrained B-Splines"
author: "Michael Chirico"
date: "February 18, 2017"
output:
  pdf_document:
    keep_tex: yes
header-includes:
  - \usepackage{theapa}
abstract: 'Most unionized teachers are paid according to a salary schedule (specifying wages as an increasing function of tenure and certification) explicated in contracts collectively bargained at the district level. With this in hand, teachers are able to infer their future potential wage trajectories at their own and other potential district employers. Lacking the physical contract faced by the teachers, an econometrician armed only with administrative data reporting actual wages in a given year must use some imputation techniques to deduce the wage structure. We explore the utility of natural Constrained B-Splines (COBS) to this end. COBS are an enhanced version of the traditional semiparametric splines technique enhanced by the ability to impose a monotonicity constraint on the resultant curve.'
bibliography: references.bib
link-citations: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, results = 'hide')
knitr::read_chunk('pay_scales_data_cleaner.R')
```

# Introduction

For many years, the ubiquitous characteristic of collectively bargained teachers' contracts has been the salary table, which gives a mapping from the calendar year, a teacher's experience (their length of tenure at the current district), and their certification (typically Master's vs. Bachelor's degree) to their wage. This table gives current teachers a clear understanding of how their pay will advance as a function of their labor inputs, and thereby gives forward-looking potential teachers and potential migrant teachers a clear understanding of their potential pay arcs under a district-switching decision-making framework, especially given that this information is typically openly available. 

It would behoove an econometrician seeking to understand education labor market dynamics, then, to incorporate this information on future pay into their statistical modeling framework. Unfortunately, this data is typically not available in a format lending itself to easy analysis at scale -- whether locked inside idiosyncratically formatted and sporadically-available contract PDFs or hidden behind large-scale freedom of information act inquiries, the temporal and financial costs of scraping such data into a usable form can be substantial.

Much more common in empirical settings is access to teacher-year-level salary data of the form $y_{i, t} = y(\tau_{i, t}, c_{i, t}, d_{i, t}) + \varepsilon_{i, t}$, where $\tau_{i,t}$, $c_{i, t}$ and $d_{i, t}$ are the tenure, certification, and district of teacher $i$ in year $t$, and $\varepsilon_{i, t}$ represents unaccount factors affecting the wage (e.g., not all teachers work full time, and many teachers supplement their income with additional duties like coaching). The goal of this paper is to give one approach and some empirical lessons for trying to estimate the underlying mapping $y(\tau, c, d)$ from such data.

# Method

```{r start_read, cache = TRUE, include = FALSE}
```

There are a multitude of inference/imputation techniques suitable to the inference of a latent function of unknown parametric form available in the statistician/econometrician's palette. The powerful flexibility of nonparametric approaches (local regression, splines, Random Fourier Feature expansions) is a double-edged sword; as it happens, in this particular setting, even if we know linearity is not a reasonable functional form restriction, we do know some very basic properties of the underlying tenure-wage curves that will be violated in general by uninformed estimation techniques. In particular, we know that such tenure-wage curves are non-decreasing and that they are non-negative, i.e., $y(\tau', c, d) >= y(\tau, c, d)$ whenever $\tau' >= \tau$, and $y(0, c, d) \geq 0$.

@he introduce a linear programming approach to incorporating monotonicity, curvature, and pointwise restraints to quantile regression spline estimation techniques, and @ng present an overview of the R package \texttt{cobs} which gives an efficient implementation of this approach (COBS standing for Constrained B-Splines). The basic idea of quantile regression spline estimation is to swap out the standard squared loss function for a quantile-dependent weighted absolute loss function to target conditional quantiles instead of conditional means. Monotonicity, point, and curvature restrictions enter as penalized terms to the objective function; \texttt{cobs} expresses this in a fashion which facilitates the application of standard linear programming techniques for efficiency, and handles internally the issues of knot selection and penalty parameter assignment through cross-validation.

We implement and fine-tune this general approach with an eye to being as minimally-invasive as possible. The first innovation is required by the poor performance of standard COBS fit in extrapolation. Data sparsity in smaller districts means that it is often the case that only a small range of interesting $\tau$ values are observed in a given year-certification-district. Monotonicty constraints are only built into the B-spline routine internally; the underlying basis functions may produce decreasing fits outside the observed range of data. To overcome this, we take a cue from the literature tackling Runge's Phenomenon [@runge], wherein polynomial approximations tend to exhibit extreme oscillations in extrapolation. This issue is one of the motivations behind natural cubic/smoothing splines [see, e.g., @friedman; @wahba; @green; or @deboor], which handle this issue by using a simple linear basis function outside the outermost interpolating knots. We incorporate this technique of linear extension only when necessary by testing the COBS fit for monotonicity; $\tau$ values failing this constraint are replaced by extending the final non-decreasing fit values through the end of the range of extrapolation.

Next, a major shortcoming of COBS for this context is its limit to one-dimensional spline fits; while techniques for nonparametric B-spline fits are available in arbitrary dimensions [see @deboor], at present COBS is only capable of imposing monotonicity on one dimension of a curve. In our context, however, $y(\tau, c)$ is increasing not only with respect to $\tau$, but also with respect to $c$ (as, without fail until only very recently in Wisconsin, certification was rewarded with a Master's premium, typically a percentage increase in wage). One solution would be to generalize the implementation of COBS to handle a second dimension by simply adding penalty terms along this dimesion. We abandon this approach because of the categorical nature of the certification dimension -- there are not numerical units to the difference between having a Master's vs. Bachelor's degree; the assignment of such a number required by this approach would itself become an implementation hyperparameter, meaning that the ultimate fit would itself be sensitive to the choice of continuous representation.

Instead, we use a two-step procedure to fit the Bachelor's and Master's pay tracks in serial. In the first step, we fit the Bachelor's career track as a typical one-dimensional COBS fit. In the second step, we first construct Master's premia for each observation by subtracting out the predicted Bachelor's pay corresponding to each observed level of tenure for a teacher with a Master's degree. We then use COBS to fit a non-decreasing Master's premium curve over all tenure levels on these residuals, before finally adding the Master's premium and Bachelor's fit curves to get the overal Master's fit curve. Monotonicity of the result is guaranteed by forcing upwards monotonicity on the Master's premium, a restriction in line with the empirical observation that Master's degree pay is often simply a fixed-percentage rise over the corresponding Bachelor's pay.

# Data

```{r sample_restrictions, include = FALSE}
library(data.table) #since sourcing a chunk doesn't bring the libraries here
library(funchir) #for tile.axes
to.pct = function(x, dig = Inf) round(100 * x, digits = dig)
pn = function(x) prettyNum(x, big.mark = ',')

#use some command line utilities to automatically
#  generate some counts for the pre-cleaning counts
raw_file_pattern = 
  '/media/data_drive/wisconsin/teacher_raw_data/data_files/*.csv'
count_all_lines = 
  paste0('wc -l ', raw_file_pattern, ' | ',
         'grep total | awk \'{gsub("[^0-9]", "")}1\'')
count_headers = paste('ls', raw_file_pattern, '| wc -l')
full_N = as.integer(system(count_all_lines, intern = TRUE)) - 
  as.integer(system(count_headers, intern = TRUE))
```

The state of Wisconsin's Department of Public Instruction (DPI) releases annual Salary, Position & Demographic reports through the WISEstaff data collection system, and these reports represent "a point-in-time collection of all staff members in public schools as of the 3rd Friday of September..." [@dpi]. The crucial data elements from each report are the Highest Degree Code (certification), Total Experience in Education (tenure), Total Salary, Total Fringe/Employee Benefits, and Agency of Work Location Code (unique identifier of district). We pull data from the 1994-95 academic year (AY) through AY2015-16 for a total of `r pn(full_N)` teacher-position-year observations (many teachers serve in multiple roles within a school/district, and each of these is filed separately in the DPI system).

Data are first fed through the matching algorithm described in further detail in a companion paper. As a consequence, `r pn(full_N - NN[1L])` (`r to.pct(1 - NN[1L]/full_N, 1L)`%) observations are lost on account of belonging to teachers who could not be uniquely identified in a given year of data due to sharing a first name, last name, and birth year with another teacher in the data. Specific to the exercise at hand, with data reliability and precision in mind, we make a series of further restrictions on the data, as depicted in Figure \ref{fig:samp_res}.

```{r drop_plot, fig.cap = '\\label{fig:samp_res}Sample Restrictions'}
barmat = rbind(rev(NN), c(rev(-diff(NN)), 0))
colnames(barmat) = 
  c('Numerical Stability Flag', 'Taught Regular Subject',
    '1-30 Years\' Experience', 'Total Pay Plausible', 
    'Full-Year Employment', 'District Type', 
    'Highest-Intensity Post', 'Full-Time', 
    'Staff Category', 'MA/BA Holders', 'Teachers', 'All Staff')
par(mar = c(5.1, 6.1, 4.1, 2.1))
barplot(barmat, col = c('blue', 'red'), horiz = TRUE, las = 1L,
        main = 'Effect of Successive Restrictions on Sample Size',
        xlab = 'Sample Size', cex.names = .6)
```

To wit, from the full staff file are first eliminated all non-teaching positions; most common among the dropped observations are "Other Support Staff", "Program Aide", and "Short-term Substitute Teacher," all of whom are likely to have their pay structured in ways that differ substantially from teachers (e.g., these positions are often part of a separately-bargained contract). We then remove all teachers whose highest degree is neither a Bachelor's nor a Master's degree, a minor restriction which allows substantial clarity in understanding districts' pay structure. For more precision, we next eliminate teachers not categorized as professionals in regular education (as opposed by and large to those in special education)^[Sensitivity analyses to this and other restrictions are given in the Appendix.].

The next two restrictions are related to a teacher's full-time equivalency. Full-time equivalency is for the most part used to break down the relative use of staff's time on various duties (for example, non-teaching time), or to place substitute teachers on a pay grade commensurate with their work load. Given the possibility of split duties to dilute the contribution of a teacher's classroom role to their pay, we eliminate teachers whose full-time equivalency across teaching roles is not 100; we then select only the highest-intensity (highest-FTE) role for each teacher in a given year^[With a very small number of likely-erroneous exceptions, each teacher's observation at a given district in a given year is associated with exactly one value for their pay (so that pay is not broken down by position), which means that neither can we "scale up" less-than-full-time teachers' pay to full time values, nor is it particularly important to select the highest-intensity role for each teacher.].

The next three restrictions have only a minor effect on sample size, and they are to eliminate districts not categorized as Wisconsin Public Schools (for the most part, observations failing this test are teachers employed by CESAs, Wisconsin's supra-district administrative unit), to eliminate teachers recorded as having worked a period of time different from a full school year, and to eliminate a small number of teachers whose pay was implausibly small for a full time staff member (less than \$10,000 in a year). After this, we cut teachers outside 1-30 years' experience for clarity and precision (sample sizes begin to drop off around 25 years of teaching), after which we eliminate teachers assigned to non-teaching subjects (health, academic support, gifted & talented support, etc.).

Finally, we make a series of cuts which are either required for COBS to function, or else increase the reliability of its output substantially. The most noteworthy/far-reaching of these numerical restrictions is to eliminate any teachers working in districts where there are not at least 20 total teachers in each degree track. While ultimately arbitrary, this number is reasonable to limit the potential effect of an individual teacher given the function to be fit to 30 levels of experience with minimal functional form restrictions. The other numerical flags require both the BA & MA track to be represented at a district, for at least 7 different levels of experience to be represented within a degree track, and for at least 5 unique values of the two measures of pay (salary and fringe benefits) to be available in each degree track; any teacher at a district failing these tests is dropped.

The sum total of all of these restrictions leaves us with an analysis sample of `r pn(nrow(full_data))` teacher-year observations, made up of `r pn(n_teachers)` individual teachers in `r full_data[ , uniqueN(district_fill)]` districts over `r full_data[ , uniqueN(year)]` years. In total, there is sufficient data to fit `r pn(uniqueN(full_data, by = c('year', 'district_fill', 'highest_degree')))` $y_t(\tau, c, d)$ curves, an average of roughly `r round(nrow(full_data)/uniqueN(full_data, by = c('year', 'district_fill', 'highest_degree')), -2)` observations per curve.

```{r milwaukee, fig.cap = '\\label{fig:mwk}Pay in Milwaukee, 2003-2006'}
mwk = full_data[district_fill == 3619]
par(mfrow = c(2, 2),
    mar = c(0.1, 0.1, 0.6, 0.1),
    oma = c(5.1, 5.1, 4.1, 2.1))
yrs = 2003:2006
parm = list(x = list(),
            y = list(las = 1L))
for (ii in 1:4) {
  yr = yrs[ii]
  mwk[year == yr, 
      as.list(c(N = .N, quantile(salary, c(.25, .5, .75)))), 
      keyby = .(ba = highest_degree == 4,
                ex = total_exp_floor)
      ][CJ(ba = c(FALSE, TRUE), ex = 1:30), on = c('ba', 'ex')
        ][ , {
          ex = unique(ex)
          BA = .SD[(ba), !c(1:3), with = FALSE]
          MA = .SD[(!ba), !c(1:3), with = FALSE]
          ymx = 1.05*max(unlist(c(BA, MA)), na.rm = TRUE)
          matplot(ex, BA, type = 'l', lty = c(2, 1, 2), 
                  lwd = 1L, col = 'blue', ylim = c(0, ymx),
                  axes = FALSE)
          mtext(yr, side = 3L, cex = .6, line = -.5)
          if (ii %% 2 == 1) mtext('Salary', side = 2, line = 4)
          if (ii > 2) mtext('Tenure', side = 1, line = 3)
          matplot(ex, MA, type = 'l', lty = c(2, 1, 2), 
                  lwd = 1L, col = 'red', add = TRUE)
          tile.axes(ii, 2, 2, parm)
          par(new = TRUE)
          barplot(matrix(N, nrow = 2, byrow = TRUE),
                  beside = TRUE, col = c('red', 'blue'),
                  ylim = c(0, 2.5*max(N, na.rm = TRUE)), axes = FALSE)}]
}
legend('right', c('BA', 'MA', 'Median', '25/75 %ile', '# Teachers'),
       col = c('blue', 'red', 'black', 'black', 'black'),
       lty = c(1, 1, 1, 2, NA), pch = c(NA, NA, NA, NA, 15), bty = 'n', cex = .5)
mtext('Empirical Distribution of Salary in Milwaukee\n' %+%
        'By Certification', outer = TRUE, side = 3)
```

As an illustrative example of the patterns in the data we wish to quantify and formalize, we turn briefly now to Milwaukee Public Schools, the largest district in Wisconsin. Figure \ref{fig:mwk} depicts key moments of the empirical distribution of salary in 4 years at Milwaukee Public Schools, broken down by tenure and certification. The central lines on each plot (Bachelor's pay track in blue, Master's pay track in red) are the empirical median levels of pay, and thus give a rough approximation to $y(\tau, c)$. The dashed-line intervals on either side represent the 25th and 75th percentiles.

Notably, these intervals and the medians themselves tend to get quite noisy at later stages in the career, especially for the Bachelor's track. This fact that reflects the almost universal certification of teachers by about 15 years into their career. This is reflected in the bar graph below each set of curves, which shows the distribution of teachers in each certification track by tenure. Almost all new teachers start with only a Bachelor's degree; the relative presence of Master's degrees grows over time as more teachers certify mid-career.

We can also note two more key empirical facts from this plot. First, the vanishing presence of teachers in both certification tracks leads the empirical median to be a poor approximation of $y(\tau, c)$ since it frequently fails to respect the fundamental monotonicity constraint discussed above. With respect to tenure, this tends to affect the Bachelor's track later in the career as more teachers certify, and the Master's track very early in the career before teachers certify. The monotonicity with respect to $c$ is maintained here for Milwaukee, but this is not always the case; our estimation procedure is thus careful to impose these restrictions internally.

Second, structural breaks are an important empirical phenomenon in this context. Each time a contract is renegotiated at a district, the tenure-wage curves can potentially change shape dramatically. It is with this in mind that we refrain, given our ignorance with respect to when such structural breaks occur, from combining information from adjacent years in fitting a given year's curve, an approach which would substantially enhance the statistical power avaiable to sparsely-populated districts. Such a structural break is apparent in Milwaukee, for example, between 2003 and 2004 and between 2005 and 2006, where the shape of the Master's pay scale has shifted notably. While we eschew, for example, full Bayesian estimation of structural breaks in a given district, such techniques are applicable; instead, we settle for the purposes at hand with investigating evidence for such breaks _ex post_.

# Results

```{r scales_import}
scales = fread('/media/data_drive/wisconsin/wisconsin_salary_scales_imputed.csv')
```

Returning to the motivating example illustrated in Figure \ref{fig:mwk}, we turn first to the performance in Milwaukee, where, given the relative sample size, performance is expected to be very good. Indeed this is the case, as seen in Figure \ref{fig:mwk_fit}. The COBS fit has retained all the salient features of the empirical median return to experience and certification, while simultaneously improving over this nonparametric conditional median by ironing out nonmonotonicities found empirically as a result of small-sample bias. 

```{r milwaukee_fit, fig.cap = '\\label{fig:mwk_fit}Estimation Results for Milwaukee, 2003-2006'}
X = mwk[ , .(med_ba = median(salary[highest_degree == 4]),
             med_ma = median(salary[highest_degree == 5])),
         keyby = .(year, tenure = total_exp_floor)
         ][scales[district_fill == 3619, .(year, tenure, wage_ba, wage_ma)],
             on = c('year', 'tenure')]
par(mfrow = c(2, 2),
    mar = c(0.1, 0.1, 0.6, 0.1),
    oma = c(5.1, 5.1, 4.1, 2.1))
parm = list(x = list(),
            y = list(las = 1L))
for (ii in 1:4) {
  yr = yrs[ii]
  X[year == yr, {
    ymx = 1.05*max(c(wage_ma, wage_ba))
    y = .SD[ , !c("year", "tenure")]
    matplot(tenure, y, type = 'l', lty = c(2, 2, 1, 1), 
            lwd = 1, col = c('blue', 'red', 'blue', 'red'),
            ylim = c(0, ymx), axes = FALSE)
    mtext(yr, side = 3L, cex = .6, line = 0)
    if (ii %% 2 == 1) mtext('Salary', side = 2, line = 4)
    if (ii > 2) mtext('Tenure', side = 1, line = 3)
    tile.axes(ii, 2, 2, parm)}]
  box()
}
legend('right', c('BA', 'MA', 'Empirical Median', 'COBS Fit'),
       col = c('blue', 'red', 'black', 'black'),
       lty = c(1, 1, 2, 1), bty = 'n', cex = .5)
mtext('Comparison of Fit Schedule and Empirical Median in Milwaukee\n' %+%
        'By Certification', outer = TRUE, side = 3)
```

Perhaps more telling is the goodness of fit in minimally small districts. Four such examples are featured in Figure \ref{fig:small_fit}.

```{r small_dist_fit, fig.cap = '\\label{fig:small_fit}Estimation Results for Sparse Districts'}
par(mfrow = c(2, 2),
    mar = c(0.1, 0.1, 0.6, 0.1),
    oma = c(5.1, 5.1, 4.1, 2.1))
parm = list(x = list(),
            y = list(las = 1L))
dists = c(`Melrose-Mindoro (2008)` = 3428, 
          `Shell Lake (2013)` = 5306,
          `Waterford Union High School (2007)` = 6083,
          `Cochrane-Fountain City (2001)` = 1155)
ymx = 1.05*max(scales[district_fill %in% dists, wage_ma],
                 full_data[district_fill %in% dists, salary])
for (ii in 1:4) {
  dst = dists[ii]
  scales[district_fill == dst,
         matplot(tenure, cbind(wage_ba, wage_ma),
                 col = c('blue', 'red'), lty = 1, type = 'l',
                 axes = FALSE, ylim = c(0, ymx))]
  full_data[district_fill == dst,
            points(total_exp_floor, salary, col = 
                     ifelse(highest_degree == 4, 'blue', 'red'))]
  mtext(names(dists)[ii], side = 3L, cex = .6, line = 0)
  if (ii %% 2 == 1) mtext('Salary', side = 2, line = 4)
  if (ii > 2) mtext('Tenure', side = 1, line = 3)
  tile.axes(ii, 2, 2, parm)
  box()
}
legend('bottomright', c('COBS Fit, BA', 'COBS Fit, MA', 'Observed Pairs'),
       col = c('blue', 'red', 'black'),
       lty = c(1, 1, NA), pch = c(NA, NA, 1), bty = 'n', cex = .5)
mtext('Comparison of Fit Schedule and Observed Wages in Small Districts\n' %+%
        'By Certification', outer = TRUE, side = 3)
```

# Conclusion

# References

\bibliographystyle{theapa}
\bibliography{references}
